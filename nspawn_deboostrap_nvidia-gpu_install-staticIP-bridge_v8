### install nspawn-container with GPU access (nvidia) with more security than docker

#https://wiki.arcoslab.org/tutorials/tutorials/systemd-nspawn
#https://askubuntu.com/questions/590319/how-do-i-enable-automatically-nvidia-uvm

# further readings
#https://wiki.archlinux.org/title/Systemd-nspawn
#https://www.cocode.se/linux/systemd_nspawn.html

# NOTES:
# work with two windows, one for configuration on the host, one for the container and to keep track of variables set on bash
#
# outside container = work as root on the host
# inside container = work after login (start that on the host with 'machinectl login $VNAME', see below)
#
# change names and variables according to your wishes, if path do not suit you, change them
# as network a network bridge br0 is used, set this up with the network tools of your choice
# - e.g. systemd, bridge-utils, Network-Manager -, this is not covered in this tutorial,
# because there are too many possibilties and it depends on your basic network configuration.
# This tutorial was created on Debian (trixie, but should work for other versions as well)
# e.g. https://wiki.debian.org/SystemdNetworkd
# e.g. https://wiki.debian.org/BridgeNetworkConnections
# Bridging depends on the network tools you use on your system - mixing that will mess up things.
# As long as the bridge 'br0' works, the tutorial should work.

# If you need initially more packages, add additional packages to the debootstrap command.
# We focus here on a minimal system and try later to remove some.

### BEGIN


### define variables

# outside container

# set variables on host

ROOT=/root
HOMIE=/root
VNAME="aiml-gpu"
DEBOS="trixie"
ARCH="amd64"
NSPAWNPATH="NSPAWN_vcontainer"
OFFICIALPATH=/var/lib/machines
CONTFULLPATH=$HOMIE/$NSPAWNPATH
# physical device for bridge
PHYSDEV="enp3s0"
DNSIP=192.168.1.78
GATEWAY=192.168.1.1
HOSTLANADDRESS=192.168.1.115/24

# check values
echo "root path = $ROOT"
echo "home path for container folder = $HOMIE"
echo "cotainer name = $VNAME"
echo "Debian OS version = $DEBOS"
echo "CPU arch = $ARCH"
echo "path to snpawn container = $NSPAWNPATH"
echo "official path to nspawn container = $OFFICIALPATH"
echo "full path to nspawn container = $CONTFULLPATH"
echo "physical device on host for bridge = $PHYSDEV"
echo "local DNS server = $DNSIP"
echo "gateway im LAN = $GATEWAY"
echo "host LAN IP/BC address = $HOSTLANADDRESS"

# check values above carefully!

# actual start

# work as root - if you don't like that, replace all root commands with sudo ...
su -
apt-get update && apt-get install systemd-container debootstrap

# path where to put the container - will be linked to /var/lib/machines later
mkdir -p $CONTFULLPATH/$VNAME
if [ -h $OFFICIALPATH ]; then
  echo "$OFFICIALPATH already is a symlink, will proceed."
else
  echo "$OFFICIALPATH is not a symlink."
  if [ -d $OFFICIALPATH ]; then
    echo "$OFFICIALPATH is a directory."
    if [ -z "$(ls -A $OFFICIALPATH)" ]; then
      echo "$OFFICIALPATH is empty, will delete, and create symlink."
      rmdir $OFFICIALPATH
      ln -s $CONTFULLPATH $OFFICIALPATH  
    else
      echo "$OFFICIALPATH is not empty - check manually, will exit".
      exit 1
    fi
  fi
fi



### install base system via debootstrap
debootstrap  --arch $ARCH --variant=minbase --include=systemd-container,systemd,dbus,iproute2,net-tools,iputils-ping $DEBOS $CONTFULLPATH/$VNAME http://deb.debian.org/debian/


### network

# example of host with bridge br0, static IP, and later we apply a static IP to the container
# proceed only if you do not have already a bridge
#
# READ GUIDES FOR YOUR SPECIFIC OS how to do that (!)
#
# if you do not want to create a bridge with systemd, look for alternatives (/etc/network/interfaces, NetworkManager, bridge-utils, ...)

# static IP for host

# outside container

# check what is now - wen need a 'br0' bridge
ip -c link
ifconfig
brctl show
# if you already have a bridge 'br0' no need to perform the next steps...

# we work after offical Debian guide for systemd-networkd
# https://wiki.debian.org/SystemdNetworkd
#
# shutdown all other network creating services otherwise it can be that
# one messes up the other
# e.g. NetworkManager, /etc/network/interfaces
# however you work, just work with ONE service

# we choose systemd-networkd and systemd-resolved
apt-get install systemd-networkd systemd-resolved

# stop all other services if required
# we proceed only with systemd
# BE CAREFULL IF YOU WORK REMOTELY via ssh or similar, you may cut your connection!
# We won't cover this case here, we assume you have direct access with keyboard to the computer
systemctl stop NetworkManager.service
systemctl disable NetworkManager.service
service networking stop
systemctl disable networking.service
mv /etc/network/interfaces /etc/network/interfaces_BP

# for bridge
cat > /etc/systemd/network/br0.netdev << EOF
[NetDev]
Name=br0
Kind=bridge
EOF

# for connecting the bridge with the physical network device
cat > /etc/systemd/network/br0.network << EOF
[Match]
Name=$PHYSDEV

[Network]
Bridge=br0
EOF

# to create the LAN access itself, static IP
# ensure your route notes the MAC of the bridge to know your static IP
# same true for local DNS servers and adblockers like pihole, adguardhome
# if you prefer DHCP uncomment the DHCP entry and comment out the static rest

cat > /etc/systemd/network/lan0.network << EOF
[Match]
Name=br0

[Network]
#DHCP=ipv4
DNS=$DNSIP
Address=$HOSTLANADDRESS
Gateway=$GATEWAY
EOF

# restart network
systemctl enable systemd-networkd
systemctl restart systemd-networkd
# check whether things are still ok
ifconfig
ip -c link
ping 8.8.8.8
dig github.com
dig github.com +short #140.82.121.4


### first start of container
systemd-nspawn -D $OFFICIALPATH/$VNAME -U --machine $VNAME


# inside container

# set root passwd
passwd
# allow login from outside
echo 'pts/1' >> /etc/securetty
exit


# outside container

machinectl enable $VNAME
machinectl start $VNAME
# check for errors
machinectl status $VNAME


# inside container

# change terminal window, become root and log into container
# copy the ENV variables from the start of the script into the window
machinectl login $VNAME

# set values according to your needs
# https://ramsdenj.com/posts/2016-09-22-containerizing-graphical-applications-on-linux-with-systemd-nspawn/
DNSIP=192.168.1.78
LOCALIP=192.168.1.114
GATEWAY=192.168.1.1
BC=24
IFACE="host0"

# we block usual setup
ln -sf /dev/null /etc/systemd/network/80-container-host0.network

# create a veth network with static IP
cat > /etc/systemd/network/vethernet.network << EOF
[Match]
Name=$IFACE

[Network]
DNS=$DNSIP
Address=$LOCALIP/$BC
Gateway=$GATEWAY
EOF
# check
cat /etc/systemd/network/vethernet.network

# enable and start systemd network
systemctl enable systemd-networkd.service
systemctl start systemd-networkd.service

ip route show
ifconfig
# XXX REASON FOR DELAY???
# check - wait for some time, it seems to take time till it works...
ping $GATEWAY
ping 8.8.8.8

# hostname must be the same as above $VNAME
# $HOSTNAME = $VNAME
HOSTNAME="aiml-gpu"
LOCALDOMAIN="anicca-vijja.xx"
echo "$HOSTNAME" > /etc/hostname
hostname $(cat /etc/hostname)
# check
hostname
exit
# login again to have proper hostname at the bash prompt

# no ipv6
# does not seem to work for systemd-networkd anymore
# https://unix.stackexchange.com/questions/544749/how-to-fully-disable-ipv6-in-lxd-containers-with-systemd-networkd
cat >> /etc/sysctl.conf<< EOF
# disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
EOF
# apply
apt-get update && apt-get install procps --no-install-recommends
# we need this to stay away from ipv6
systemctl restart systemd-networkd.service && sysctl -p && ifconfig
# or
# echo 1 > /proc/sys/net/ipv6/conf/$IFACE/disable_ipv6 


# outside container

# we need IP forward and the bridge mode for iptables

# kernel module
modprobe br_netfilter 
lsmod | grep netfilter
# 'br_netfilter' and 'bridge' should appear

# ONLY IF not present!
cat /etc/sysctl.conf | grep forward
cat /etc/sysctl.conf | grep bridge
cat >> /etc/sysctl.conf << EOF
# sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 1
# for netfilter on bridge
net.bridge.bridge-nf-call-iptables = 1 
EOF
# apply
systemctl restart systemd-networkd.service && sysctl -p

# create basic config for systemd-nspawn container
# this is for NVIDIA
# AMD and Intel Arc users have to adjust the 'Bind' commands
# to match their GPUs
mkdir -p /etc/systemd/nspawn
if [ ! -f "/etc/systemd/nspawn/$VNAME.nspawn" ]; then
  cat > /etc/systemd/nspawn/$VNAME.nspawn << EOF
[Exec]
PrivateUsers=0
#xhost
Environment=DISPLAY=:0.0
#Xephyr
#Environment=DISPLAY=:1

[Network]
Private=yes
VirtualEthernet=yes
Bridge=br0

[Files]
#xhost
BindReadOnly=/tmp/.X11-unix
BindReadOnly=/home/leo/.Xauthority
#xephyr
#BindReadOnly=/tmp/.X11-unix/X1
# Also necessary for Intel (maybe AMD):
#Bind=/dev/dri # Also necessary for Intel (maybe AMD)
Bind=/dev/nvidia0
Bind=/dev/nvidia1
Bind=/dev/nvidiactl
Bind=/dev/nvidia-modeset
Bind=/dev/nvidia-uvm
Bind=/dev/nvidia-uvm-tools
Bind=/dev/nvidia-caps
Bind=/dev/input
Bind=/dev/shm
#Bind=/dev/input/js0
EOF
else
  echo "/etc/systemd/nspawn/$VNAME.nspawn exists - please check manually."
fi

machinectl reboot $VNAME
machinectl status $VNAME
if [ ! -h "$OFFICIALPATH/$VNAME.nspawn" ]; then
  echo "symlink $OFFICIALPATH/$VNAME.nspawn does not exist, will create it."
  ln -s /etc/systemd/nspawn/$VNAME.nspawn $OFFICIALPATH/$VNAME.nspawn
else
  echo "symlink $OFFICIALPATH/$VNAME.nspawn already exists, check manually pointer."
  ls -la $OFFICIALPATH/$VNAME.nspawn
fi

# Mounting folders from host
# see config file above `/etc/systemd/nspawn/...nspawn`
# and https://www.freedesktop.org/software/systemd/man/latest/systemd.nspawn.html
# one can mount more folders below the '[files]' section
# Be aware that you should mount as read-only unless you really want and have to write to it.
# e.g.

[Files]
# bind read-only, e.g. AI/ML models
BindReadOnly=/$PATH-ON-HOST:/$PATH-INSIDE-CONTAINER
# bind read-write, e.g. output of AI/ML image generation
# here path on host is the same as path inside the container
Bind=/$PATH-ON-HOST

#Extend this in accordance to your needs.


# inside container
machinectl login $VNAME

# we use static DNS
DNS1=192.168.1.78
DNS2=192.168.1.64
LANNAME="anicca-vijja.xx"

cat > /etc/resolv.conf << EOF
nameserver $DNS1
nameserver $DNS2
search $LANNAME
EOF
# check
cat /etc/resolv.conf
chmod -w /etc/resolv.conf
ls -la /etc/resolv.conf

# check what is written for DNS, we drop this later
cat /etc/resolv.conf

# we do not want this changed by any process, so we make it r/o
# there are better options, but it works
# normally /etc/resolv.conf is a link to
# /run/systemd/resolve/resolv.conf managed by systemd-resolved
chmod -w /etc/resolv.conf
ls -la /etc/resolv.conf

# check
ping 8.8.8.8
ping google.de


# outside container

# more security stuff
machinectl stop $VNAME
# own it by root on host and restrict the container
systemd-nspawn -D $OFFICIALPATH/$VNAME --private-users=0 --private-users-chown --machine $VNAME
exit
# if not done yet
ls -la $OFFICIALPATH/$VNAME
# should be root, otherwise do
#chown root:root $OFFICIALPATH/$VNAME

# check, we use 'less', quite some output
# if we don't have less just do
# apt-get install less --no-install-recommends
ls -Ralh $OFFICIALPATH/$VNAME | less

# we work with NVIDIA
# prepare entries below /dev for NVIDIA entries
# AMD + Intel Arc users have to find equivalents for their GPUs
# important is that you have all entries below /dev
if [ ! -f "$ROOT/preparenspawngpu" ]; then
  echo "$ROOT/preparenspawngpu does not exist, will create it."
  cat > $ROOT/preparenspawngpu << EOF
#!/bin/bash

/sbin/modprobe nvidia

if [ "$?" -eq 0 ]; then
  # Count the number of NVIDIA controllers found.
  NVDEVS=`lspci | grep -i NVIDIA`
  N3D=`echo "$NVDEVS" | grep "3D controller" | wc -l`
  NVGA=`echo "$NVDEVS" | grep "VGA compatible controller" | wc -l`

  N=`expr $N3D + $NVGA - 1`
  for i in `seq 0 $N`; do
    mknod -m 666 /dev/nvidia$i c 195 $i
  done

  mknod -m 666 /dev/nvidiactl c 195 255

else
  exit 1
fi

/sbin/modprobe nvidia-uvm

if [ "$?" -eq 0 ]; then
  # Find out the major device number used by the nvidia-uvm driver
  D=`grep nvidia-uvm /proc/devices | awk '{print $1}'`
  mknod -m 666 /dev/nvidia-uvm c $D 0
else
  exit 1
fi

# required if the screen is not connected to the nvidia GPU(s) and uses e.g. a iGPU
mknod -m 755 /dev/nvidia-caps c $(cat /proc/devices | grep nvidia-caps | awk '{print $1}') 80
mknod -m 755 /dev/nvidia-uvm-tools c $(cat /proc/devices | grep nvidia-uvm | awk '{print $1}') 80

## that's how it should look like on the host
## required IF GPU is not used for the screen but just for number crunching
## then while booting not all entries below /dev are created
##
## ls -la /dev|grep nvidia
#crw-rw-rw-   1 root root    195,     0 17. Jul 12:14 nvidia0
#crw-rw-rw-   1 root root    195,     1 17. Jul 12:14 nvidia1
#drwxr-xr-x   2 root root            80 17. Jul 12:21 nvidia-caps
#crw-rw-rw-   1 root root    195,   255 17. Jul 12:14 nvidiactl
#crw-rw-rw-   1 root root    195,   254 17. Jul 12:14 nvidia-modeset
#crw-rw-rw-   1 root root    235,     0 17. Jul 12:14 nvidia-uvm
#crw-rw-rw-   1 root root    235,     1 17. Jul 12:21 nvidia-uvm-tools
EOF
else
  echo "$ROOT/preparenspawngpu already exists, check manually:"
  cat $ROOT/preparenspawngpu
fi

# apply
chmod +x $ROOT/preparenspawngpu
$ROOT/preparenspawngpu
# look for any error messages
# check
ls -la /dev|grep nvidia


# inside container

# if ever you are not logged in or it does not run, do as root:
# copy variables from start of the script
machinectl start $VNAME
machinectl login $VNAME

apt-get update
apt-get install curl gpg ca-certificates --no-install-recommends

#https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# if you need the deb-multimedia repo, do the following
# https://deb-multimedia.org/
#curl -o deb-multimedia-keyring_2016.8.1_all.deb https://www.deb-multimedia.org/pool/main/d/deb-multimedia-keyring/deb-multimedia-keyring_2016.8.1_all.deb
#dpkg -i deb-multimedia-keyring_2016.8.1_all.deb
#rm deb-multimedia-keyring_2016.8.1_all.deb

ARCH=amd64
LOCALMIRROR="ftp2.de.debian.org"
# add $LOCALMIRROR to your $VNAME_BP/whitelisteddomains.txt

cat > /etc/apt/sources.list << EOF
deb http://$LOCALMIRROR/debian/ trixie main non-free contrib non-free-firmware
deb http://$LOCALMIRROR/debian/ trixie-backports main contrib non-free non-free-firmware
deb http://$LOCALMIRROR/debian/ trixie-updates main contrib non-free non-free-firmware
deb http://security.debian.org/debian-security trixie-security main contrib non-free non-free-firmware
# uncomment this if you need it
#deb https://www.deb-multimedia.org trixie main non-free
EOF


# outside container

# VIP! the nvidia driver must be the same version like on the host
dpkg -l | grep nvidia


# inside container

# nvidia driver + browser
apt-get update && apt-get install nvidia-driver nvidia-cuda-toolkit nvidia-smi nvtop firefox-esr --no-install-recommends
# if it ever does not proceed, breact with CTRL-C and restart the command
# at some point it will ask you to configure your keyboard, just follow instructions on the screen

# create users, change according to your needs
# one user for AI/ML stuff
USERAI="aiml"
adduser $USERAI
# one user just to use the browser
USER="browser"
adduser $USER


# outside of container

# enable/ allow nvidia stuff
systemctl edit systemd-nspawn@aiml-gpu.service 
# copy the following into the part where it is stated that configs should be out - at the beginning:
[Service]
DeviceAllow=/dev/nvidiactl
DeviceAllow=/dev/nvidia0
# if you have more than one nvidia GPU
#DeviceAllow=/dev/nvidia1
DeviceAllow=/dev/nvidia-modeset
DeviceAllow=/dev/nvidia-uvm
DeviceAllow=/dev/nvidia-uvm-tools
DeviceAllow=/dev/nvidia-caps
DeviceAllow=block-loop rwm
# save the file and exit editor

# you can also write to /etc/systemd/system/systemd-nspawn@aiml-gpu.service.d/override.conf
cat /etc/systemd/system/systemd-nspawn@aiml-gpu.service.d/override.conf

# reboot machine
machinectl reboot $VNAME


# inside container

machinectl login $VNAME

# login as user $USERAI

# check for nvidia, must give some meaningful output about accessible nvidia GPUs
nvidia-smi
nvtop

# outside container as desktop user (not root!)
# never do just 'xhost +' - this would enable *everyone* to connect to you
xhost +local:


# inside container

machinectl login $VNAME

# login as user $BROWSER

# before starting any X application it requires to set $DISPLAY!
# we will make it better later, but to test it is ok to use xhost
# xhost method
export DISPLAY=:0.0
firefox-esr

# alternative with xephyr (=nested X server)
# outside container

apt-get install xserver-xephyr xdotool

# in /etc/systemd/nspawn/$VNAME.nspawn replace

Environment=DISPLAY=:0.0

by

#Environment=DISPLAY=:0.0
Environment=DISPLAY=:1

and

BindReadOnly=/tmp/.X11-unix
BindReadOnly=$HOMIE/.Xauthority

by

# xhost/ xauth
#BindReadOnly=/tmp/.X11-unix
#BindReadOnly=/home/leo/.Xauthority
# xephyr
BindReadOnly=/tmp/.X11-unix/X1

# save file and exit editor

# start nested X-server use user on the host
# adjust the screen size to your local needs
#Xephyr :1 -resizeable
# run it with disabled shared memory (MIT-SHM) and disabled XTEST X extension
# access is via unix socket /tmp/.X11-unix/X1
# one can also disable tcp
Xephyr -ac -extension MIT-SHM -extension XTEST -nolisten tcp -screen 1920x1080 -br -reset :1
Xephyr -ac -extension MIT-SHM -extension XTEST -screen 1920x1080 -br -reset :1
# a new window pops up, change to another terminal as root

# if you cannot get the DISPLAY:1 in the container, reboot it
machinectl reboot $VNAME


# inside container as $BROWSER

machinectl login $VNAME

export DISPLAY=:1
firefox && xdotool search --onlyvisible --class Firefox windowsize 100% 100%

# now it's basically ready to be used for AI/ML engines like comfyui, fooocus, stable swarm, etc.
# install AI/ML engines as $USERAI
# start with minoconda or some virtual python env

# use AI/ML stuff as user $USERAI
# use the browser as user $BROWSER
# that separates the users from each other

# as root
# install git required for AI/ML engines
apt-get install git --no-install-recommends


# inside container

### installation miniconda + ComfyUI + ComfyUI-Manager

# log in as $USERAI

# check webpages always for latest install

# miniconda installation
# https://docs.anaconda.com/miniconda/#miniconda-latest-installer-links
curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o ~/Miniconda3-latest-Linux-x86_64.sh
chmod +x ~/Miniconda3-latest-Linux-x86_64.sh
# for install use default values, it is ok to change the behavior that miniconda is "active" after log in
~/Miniconda3-latest-Linux-x86_64.sh
# log out and log in as $USERAI
# create python env
conda create --name comfyui-exp python=3.11
# activate python env
conda activate comfyui-exp


### before installing AI/ML engines, we do some more SECURITY

# outside container

# THIS ENTRY MUST MATCH THE ACTUAL IP of the container

# manual steps for a single whitelisted domain below
# for more use the script for iptables + etc-hosts and run it as root.
# latest scriptname - adjust path where you have copied it
$ROOT/NSPAWN_iptables-etc_v3
# if you ever cannot reach a domain:
# - add it manually to the whitelist at $VNAM_BP/whitelisteddomains.txt
# - re-run the script
# - try again


# inside container as $USERAI

# ComfyUI + ComfyUI-Manager
cd
git clone https://github.com/comfyanonymous/ComfyUI.git
cd ~/ComfyUI
# have a look at python requirements BEFORE install
cat requirements.txt
cd ~/ComfyUI/custom_nodes
git clone https://github.com/ltdrdata/ComfyUI-Manager.git
cd ComfyUI-Manager
# have a look at python requirements BEFORE install
cat requirements.txt
cd ../..
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121 --dry-run
# you can check for malware after EACH --dry-run
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt --dry-run
# check downloaded files...
pip install -r requirements.txt
cd custom_nodes/ComfyUI-Manager
pip install -r requirements.txt --dry-run
# check downloaded files...
pip install -r requirements.txt
cd ../..
# start ComfyUI
python main.py


# log in separately (different terminal) as user $BROWSER
# xhost method
export DISPLAY=:0.0
firefox-esr
# go to ComfyUI-Manager, install a SD model, and use the default template to create an image
# if that works, the rest will work as well...



# whitelisted domains for daily usage should be (ComfyUI)
#
# github.com 		# code, be aware MALWARE can come from here!
# raw.githubusercontent.com	# code, be aware MALWARE can come from here!
# huggingface.co 	# AI models (always use safetensors, not ckpts)
# civitai.com		# AI models stable diffusion (always use safetensors, not ckpts)
# pypi.org		# python stuff (check and do 'pip install ... dry-run' before applying
# dowload.pytorch.org	# pyTorch
# files.pythonhosted.org # python / pip
# debian mirrors + security.debian.org + deb.debian.org 	# updates and security fixes
# whatever is essential to run AI/ML engines
# [...]

# there is no reason to allow for more from within the container
# even models can be downloaded manually from civitai or huggingface and
# checked by malware/ virus scanner unless you really want ComfyUI-Manager to do that
# then you have to keep track of whitelisted domains
# better keep the whitelist simple and short

# we do not cover here the usage of malware scanners, rootkit detectors, and audits
# that requires in-depth knowledge of systems, esp. because **detection does not mean hardening**
#
# just some links under Linux:
#
# https://linuxsecurity.expert/security-tools/linux-malware-detection-tools
# https://www.tecmint.com/scan-linux-for-malware-and-rootkits/
# https://www.tecmint.com/install-linux-malware-detect-lmd-in-rhel-centos-and-fedora/
# https://www.digitalocean.com/community/tutorials/how-to-sandbox-processes-with-systemd-on-ubuntu-20-04

# clamav		# virus scanner
# rkhunter		# rootkit detection
# lynis			# audit tool of system tools, can be applied to container as well
# chkrootkit		# check for rootkits
# LMD			# linux malware detect
#
# and for the people who know what they do.. btw - not everything is FOSS and this is just a list,
# not a recommendation:
#
# AIDE			#
# maltrail		#
# ossec			#
# openvas		#
# radare2		#
# remnux		#
# rkdetector		#
# tiger			#
# tripwire		#
# vuls			#
# yara			#

# for those again who are from the IT world 'apparmor' and 'SELinux' are good tools to
# create an additional layer of security. 'firejail' can be compiled with 'apparmor' support.

# check the net for what is possible, use several parallel to each other
# use cron-jobs to check on the container from the host each night
# let the cron-job send you a local mail to your admin account with a summary
# check after each install of plugins, models, etc. BEFORE starting your AI/ML engine
#
# use common sense, have a look at the python code, the requirements, etc.,
# and check what is downloaded and from where

# This is no guarantee to be malware free, but it makes it harder to be infected, and
# with a container it is harder to infect your whole system, get data, etc.

# make a backup of the container before starting with anything
# in case of infection, just wipe the container, boot from external live system
# check the host carefully, and only then restart with the backup of the container

### END



### notes about browsers

# inside container as root

# browser installations downloads a lot of stuff on your system...
# so either restrict the browser via e.g. firejail/ flatpak/ snap or accept that it installs a lot of stuff

# example - just a check what should be done BEFORE install
apt-get install firefox-esr
# break if you do not want to do that... count the number of packages...

# you can restrict packages with
# apt-get install --no-install-recommends [packages]
# but check that all dependencies are given, if something fails, install the missing libraries/ tools

# start the browser later ALWAYS as $USER and never as $ROOT or $USERAI to keep things separately
# $ROOT is only for system administration
# some browsers like tor tell you it is not a good idea to play browser and root at the same time


# some browser examples under Linux

# netsurf
apt-get install netsurf-gtk

# palemoon https://www.palemoon.org/download.shtml
apt-get install libdbus-glib-1-2
# check for latest version before downloading
wget http://linux.palemoon.org/datastore/release/palemoon-32.0.0.linux-x86_64-gtk3.tar.xz
tar -xvf palemoon-32.0.0.linux-x86_64-gtk3.tar.xz
# as $USER
./palemoon/palemoon # as user

# midori
https://astian.org/midori-browser/download/linux/
# download manually and install from that directory
dpkg -i midori_11.3.3_amd64.deb
# if it complains about missing libs, uncomment the following to install them
#apt-get -f install
# there are incidents when the browser version really does not fit to the OS, then drop that or invest time...
# we want to keep it simple, we won't go to the net, we just need the browser for local access to
# AI/ML engines, so no need to invest much
# as $USER
midori

# vivaldi
# https://www.vivaldi.com
# check for latest version
wget https://downloads.vivaldi.com/stable/vivaldi-stable_6.8.3381.48-1_amd64.deb
dpkg -i vivaldi-stable_6.8.3381.48-1_amd64.deb
# if required
#apt-get -f install
# as $USER
vivaldi

# firefox
apt-get install firefox-esr
# as $USER
firefox-esr


# note about snap and flatpak as alternative sources for browser installation
#
# or use firejail along with apparmor (requires compilation, add profiles, etc.)
# https://firejail.wordpress.com/download-2/ and https://github.com/netblue30/firejail

# or there are additional possibilities with systemd sandboxing
https://www.digitalocean.com/community/tutorials/how-to-sandbox-processes-with-systemd-on-ubuntu-20-04

# One can install browsers via snap and flatpak, whether this is more secure is a good question.
# There are notes on https://flatkill.org/ that doubt the overall security of flatpak
# and there were two cases of malware on snap but this is years ago and was detected rather quickly.
# So this is no real reason against snap, rather it shows they seem to care about malware.
# However, the snap store itself is closed source.
# snap originated from canonical and flatpak originated from redhat.
# As usual it is a matter of trust and trustworthiness of repositories.
# As long as we need only a browser for a webUI, tor directly from tor is a valid choice as they
# care about regular updates that are shown asap from within the browser.
# We do not need more for webUIs.
# As debian hosts a complete repo system there is no real need to abbreviate from the repos
# unless you take a browser directly from the manufacturer and you have to trust them
# So the choice is up to you.
#
# Same true for nvidia closed source repo drivers, but we have no other choice at the moment.


### some more restrictions

# check for security updates
# and get rid of unnecessary stuff...
# do this when you have installed everything you need or reverse the steps later

# https://phoenixnap.com/kb/automatic-security-updates-ubuntu
apt-get update
apt-get install unattended-upgrades --no-install-recommends

# use your editor fo choice
# install an editor of your choice: joe, nano, vi, ... put in the name
EDITORNAME="joe"
EDITOR=$(which $EDITORNAME)
$EDITOR /etc/apt/apt.conf.d/50unattended-upgrades
# comment out:

//        "origin=Debian,codename=${distro_codename},label=Debian";

# we want only security updates which we want automatically
systemctl restart unattended-upgrades
systemctl status unattended-upgrades


# outside container

# we reduce our /etc/apt/sources.list to nvidia stuff and security updates
# be aware that you may have to reverse that in case nvidia updates would require other system libraries
# to be updated, then add the original sources.list, so we need to back it up.
cp $OFFICIALPATH/$VNAME/etc/apt/sources.list $ROOT/$VNAME_etc-apt_sources.list_full-BP
# restore it with
#cp $ROOT/$VNAME-etc-apt_sources.list_full-BP $OFFICIALPATH/$VNAME/etc/apt/sources.list
#
# reduced sources.list - only nvidia and security updates
# look whether localmirror is still ok
echo $LOCALMIRROR

# in $OFFICIALPATH/$VNAME/etc/apt/sources.list
# comment out all lines for "main", "-backports", and "-updates'
# if you want only security fixes and nothing else

#deb http://$LOCALMIRROR/debian/ trixie main non-free contrib non-free-firmware
#deb http://$LOCALMIRROR/debian/ trixie-backports main contrib non-free non-free-firmware
#deb http://$LOCALMIRROR/debian/ trixie-updates main contrib non-free non-free-firmware
deb http://security.debian.org/debian-security trixie-security main contrib non-free non-free-firmware
# uncomment this if you need it
#deb https://www.deb-multimedia.org trixie main non-free

# This will allow only security updates, be aware to install something else you have to re-enable
# we can secure this against overwriting

# make a backup
cp $OFFICIALPATH/$VNAME/etc/apt/sources.list $ROOT/$VNAME_etc-apt_sources.list_red-BP
# restore
#cp $ROOT/$VNAME_etc-apt_sources.list_red-BP $OFFICIALPATH/$VNAME/etc/apt/sources.list
# make it really r/o with 'chattr'
chattr +i $OFFICIALPATH/$VNAME/etc/apt/sources.list


# note on network and malware scans
#
# as long as AI/ML webUIs do not implement strict security measures (almost impossible for them to do all!)
# you can work the following, e.g. ComfyUI + ComfyUI-Manager - the steps below could partially be added
# to ComfyUI-Manager
#
# -1-link as read-only your $COMFYUIROOT/models folder of ComfyUI into the container via a BIND rule if
#    you do not want all models within the container
#
# 0- enable network
#    on host:
#    IFACE = "vb-$CONTAINERNAME"
#    ip link set $IFACE up
#
# 1- install plugins manually with 'git clone [...]',
#    but before doing anything check the ComyUI-Manager because they start to add security checks
#    there to mark established and therefor more trustworthy plugins
#
# 2- go the folder of the plugin, have a look at the requirements.txt of python
#
#    cat './requirements.txt'
#
#    And see for yourself what should be installed and whether it looks suspicious or unusual.
#    If yes, search for those python packages whether they exist and are official incl. version number.
#    This is important, because even if it is no malware it can be mess up your venv esp. if a
#    lot of python packagess with specific versions must be installed - it can break your
#    conda/venv env (!) so sometimes it makes sense to clone the env and try out on the clone
#    before messing up your productive env.
#
# 3- install first via dry-run
#
#    pip install -r requirements.txt --dry-run
#
#    This downloads the requirements but does not installs anything,
#    Run from the host a malware/ virus scan on the files/ folder(s),
#    Only then install them (no download required, files are already present).
#    DO NOT restart ComfyUI but shut it down.
#    
# 4- disable network on the host
#
#    ip link set $IFACE down
#
#    Restart ComfyUI and check whether a plugin requires to download a model or whatever.
#    Check that carefully and re-enable network to do that.
#    Restart ComfyUI and check whether the same message reappears or anything starts as expected.
#
# 5- check with the script 'xxx' regularly that your whitelisted domains are up-2-date and you have
#    access to all domains you need from within the container, but to nothing else.
#
# 6- BE AWARE that github is not a secure repository, but you need it for AI/ML stuff (!)
#    e.g. https://www.heise.de/en/news/Ghost-accounts-on-GitHub-Organized-malware-slinger-with-a-system-9813416.html
#    even if such accounts are removed a repo can be used to distribute malware for some time and a
#    AI/ML plugin (intentionally or not!) can be infected that way
#
# So in sum the security measures are only partially even if the manual effort above may look like "nonsense/ overkill."
# A container without network cannot do that much, but that does not mean to disable common sense.
# Regular scans and reading on official github repos/ reddit groups/ etc. are good to receive information about
# malware as soon as possible.
#
# IF one can automate/script some of the parts here for daily security, just go ahead!
#
# LAST - make regular backups of the container on ext HDD, different computer/ NAS, etc.
# In case of infection just wipe it completely, shut down the host, check the host via live system
# intensively with several malware scanners for infection.
# After restart just copy the backup of the AI/ML engine and find out what caused the infection so
# that is not repeated again.
# Inform others if you are the first one infected and even if not, confirm if others experienced the same.

### END


# whitelisted domains to install everything above
# cat $VNAME_BP/whitelisteddomains.txt
debian.org
debian.map.fastlydns.net
ftp2.de.debian.org
security.debian.org
nvidia.github.io
github.com
raw.githubusercontent.com
huggingface.co
pypi.org
download.pytorch.org
files.pythonhosted.org

# cat /var/lib/machines/aiml-gpu/etc/hosts
127.0.0.1	localhost
127.0.1.1	aiml-gpu.anicca-vijja.xx aiml-gpu

# automatically generated whitelisted domains:
151.101.194.132 debian.org
151.101.130.132 debian.org
151.101.66.132 debian.org
151.101.2.132 debian.org
146.75.118.132 debian.map.fastlydns.net
137.226.34.46 ftp2.de.debian.org
151.101.2.132 security.debian.org
151.101.130.132 security.debian.org
151.101.66.132 security.debian.org
151.101.194.132 security.debian.org
185.199.110.153 nvidia.github.io
185.199.111.153 nvidia.github.io
185.199.108.153 nvidia.github.io
185.199.109.153 nvidia.github.io
140.82.121.4 github.com
185.199.109.133 raw.githubusercontent.com
185.199.108.133 raw.githubusercontent.com
185.199.111.133 raw.githubusercontent.com
185.199.110.133 raw.githubusercontent.com
3.160.150.119 huggingface.co
3.160.150.50 huggingface.co
3.160.150.7 huggingface.co
3.160.150.2 huggingface.co
151.101.0.223 pypi.org
151.101.192.223 pypi.org
151.101.128.223 pypi.org
151.101.64.223 pypi.org
18.239.83.16 download.pytorch.org
18.239.83.126 download.pytorch.org
18.239.83.69 download.pytorch.org
18.239.83.32 download.pytorch.org
146.75.116.223 files.pythonhosted.org

